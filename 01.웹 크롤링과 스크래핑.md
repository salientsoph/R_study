- library(): 현재 R에 설치된 패키지 리스트를 나타냄
- R에는 엑셀 문서를 읽어오는 api가 없으므로, -> library(readxl) 을 사용
- 



# 정적 스크래핑

1. 웹 스크래핑(web scraping) : 

   - 끌어와서 컨텐츠 내용 중 __일부분만 뽑아오는 것__ (태그의 속성들 등)

   - 웹 사이트 상에서 원하는 부분에 위치한 정보를 컴퓨터로 하여금 자동으로 추출하여 수집하는 기술

2. 웹 크롤링(web crawling) : 

   - __끌어오는 것__
   - 자동화 봇(bot)인 웹 크롤러가 정해진 규칙에 따라 복수 개의 웹 페이지를 브라우징 하는 행위

![스크래핑](C:\Users\salient\Pictures\스크래핑.png)



![스크래핑2](C:\Users\salient\Pictures\스크래핑2.png)









## 1. 네이버 영화 사이트 댓글정보 스크래핑

네이버 영화 사이트의 데이터 중 __영화제목, 평점, 리뷰__만을 추출하여 CSV 파일의 정형화된 형식으로 저장

![네이버영화](C:\Users\salient\Pictures\네이버영화.png)

1. 스크래핑하려는 웹페이지의 URL 구조와 문서 구조를 파악해야 한다.

- 영화 제목으로 찾아본다 -> Id 속성을 본다 -> class 속성을 본다(다른 것도 같은 값이 있을 수 있어서 id속성보단 추천되지 않음)

- URL 구조 : [http](http://movie.naver.com/movie/point/af/list.nhn?page=1)[://movie.naver.com/movie/point/af/list.nhn?page=](http://movie.naver.com/movie/point/af/list.nhn?page=1)[1](http://movie.naver.com/movie/point/af/list.nhn?page=1)

- Em태그(emphasize(강조)) : div 태그에 자식태그 em 을 찾기

- css 선택자 

  - 리뷰만 골라내는 건 없음 
  - __태그__까지는 접근 가능
  - 몇 번째 컨텐츠를 꺼내겠다~ 는 기능 없음

  -> 따라서 xpath로 꺼낸다

- 문서 구조(접근하고자 할 때) 

   영화 제목 class=”.movie” (css 선택자는 . 를 붙임)

   영화 평점 class=”.title em” (title이라는 속성에 em 자식)

   영화 리뷰 xpath= ”//*[@id=＇old_content＇]/table/tbody/tr/td[2]/text()”

- css: copy > copy selector

- xpath: copy > xpath selector

- 함수

  __read_html(대상(url), encoding)__: url을 끌어오는 함수  

  - encoding의 default: UTF-8 (동일시 생략 가능)

  __html_nodes(대상, "css주소")__: 태그 선택하는 함수 

  - 2번째 인자 default: css 이므로 xpath를 줄 시에는 xpath="주소"  로 준다 

  __html_text(대상)__: 출력해주는 함수
  
  __html_attr(대상, "속성")__: 속성을 출력해주는 함수

- XPath 설명:   https://ko.wikipedia.org/wiki/XPath





[단일 페이지 스크래핑]

```R
install.packages(“rvest”); 
library(rvest)

# 이전의 설정값이 있을까봐 초기화
text<- NULL; title<-NULL; point<-NULL, review<-NULL; page=NULL
url<- "http://movie.naver.com/movie/point/af/list.nhn?page=1"

# read_html(): url을 끌어오는 함수
text <- read_html(url,  encoding="CP949"); text   # euc-kr == cp949

# 영화제목
# html_nodes(대상, css): 태그 선택하는 함수()
# html_text(): 출력해주는 함수
nodes <- html_nodes(text, ".movie")
title <- html_text(nodes); title

# 영화평점
nodes <- html_nodes(text, ".title em")
point <- html_text(nodes); point

# 영화리뷰 
nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
# trim=TRUE: text area (앞뒤에 있는 blank, 공백, tab, 개행문자 등)을 다 제거해줌
nodes <- html_text(nodes, trim=TRUE)
review <- nodes[nchar(nodes) > 0] # nodes들중 0보다 큰 것들만 남김(내용 있는 것만 남김)
review
page <- data.frame(title, point, review)
# csv로 데이터 새로 저장
write.csv(page, "movie_reviews.csv")

```









copy > copy selector

//*[@id="old_content"]/table/tbody/tr[1]/td[2]/a[1]

#old_content > table > tbody > tr:nth-child(1) > td.title > a.movie.color_b

#old_content > table > tbody > tr:nth-child(2) > td.title > a.movie.color_b

#old_content > table > tbody > tr:nth-child(3) > td.title > a.movie.color_b



#old_content > table > tbody > tr:nth-child(3) > td.title > a.movie.color_b

----------------------------------------------------> .movie



#old_content > table > tbody > tr:nth-child(1) > td.title > div > em

----------------------------------------------------> td.title em



copy > copy xpath

//*[@id="old_content"]/table/tbody/tr[1]/td[2]/text()

//*[@id="old_content"]/table/tbody/tr[2]/td[2]/text()

----------------------------------------------------> //*[@id="old_content"]/table/tbody/tr/td[2]/text()













rvest 패키지: 









xml 패키지: 

끌어오고자 하는 웹페이지가 공공db를 끌어오거나, sns api를 이용해 끌어온다거나 등은 xml과 json이 많다. 





//*[@id="main-top01-scroll-in"]/div[2]/div/h4/a





httr 패키지: get방식, post 방식 요청/ 받아오고나선 rvest 등 사용/ 스크래핑 보다는 크롤링에 특화됨

post 방식 요청

get 방식 요청(로그인, 회원가입)







open API를 활용한 공공DB와 SNS 데이터 수집

정적 웹페이지 수집과 사용이 거의 비슷



open API: 

1) 함수, 클래스, ... : API는 application programming interface(어플 개발시 가져다 쓸 수 있는, 미리 만들어져있는 함수나 클래스)

여기서 open --> free 무료로 사용하는 API

2) URL 문자열(web 세상): 필요한 데이터를 요청하고 받아갈 수 있게 지원하는 URL 문자열

- 인증키, Query 문자열 등을 필요로 함

- 요청 헤더에 규격정보를 제공해야 할 수도 있음

  



공공DB: 정부에서 지원하는 데이터베이스(서울시 빅데이터 등)

SNS: 페이스북, 트위터, 네이버, 다음 등에 올라와있는 글에 대한 수집





# 동적 스크래핑

동적 웹페이지 수집



javascript에 의한 동적 태그, ajax에 의한 동적 태그 -> 셀레늄(?)이라는 프로그램으로 끌어와야

"브라우저가 렌더링": 태그에 맞춰 그 도큐먼트를 보여줌, js 코드를 실행시켜서 컨텐츠를 동적으로 만들어낸 것 등



js를 안쓰고 html, css만 -> 동일

js를 사용, 일부 컨텐츠를 동적으로 -> 서버로 부터 보내진 소스정보랑, 렌더링된 소스 내용이 다름 



